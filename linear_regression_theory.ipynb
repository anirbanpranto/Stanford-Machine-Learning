{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "linear_regression_theory.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM+zu9iQSduqdSBhDmLIhwj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anirbanpranto/Stanford-Machine-Learning/blob/main/linear_regression_theory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Olr6RHE_Nvr"
      },
      "source": [
        "# **Linear Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3h2jnOq9_ML1"
      },
      "source": [
        "Linear Regression is a simple supervised learning algorithm <br>\n",
        "\n",
        "#### **Notation**\n",
        "$m : $ number of training examples <br>\n",
        "$x's : $ input variables/features <br>\n",
        "$y's : $ output variable/target <br>\n",
        "\n",
        "$(x,y)$ is one training example <br>\n",
        "$(x^{(i)}, y^{(i)})$ is $ith$ example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QN6I433ZAKBc"
      },
      "source": [
        "#### **Hypothesis**\n",
        "$$h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x$$\n",
        "\n",
        "Given a value $x$ the hypothesis gives us a $y$ value or prediction. We need to choose the $\\theta$ values cleverly so that we can get the best prediction/fit. For finding the optimal $\\theta$ values we use the idea of a cost function.\n",
        "\n",
        "#### **Cost Function**\n",
        "$$J(\\theta_{0}, \\theta_{1}) = \\frac{1}{2m}\\sum_{i=1}^{m}(h(x^{(i)}) - y^{(i)})^2$$\n",
        "\n",
        "Here, our goal is to minimize the value of $J(\\theta_{0}, \\theta_{1})$. It is called the Mean squared error function. We use this as our cost function to determine optimal hypothesis. In regression problems this is more popular. This is how we choose our $\\theta$ values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyTyt8Uh9Cce"
      },
      "source": [
        "\n",
        "\n"
      ]
    }
  ]
}