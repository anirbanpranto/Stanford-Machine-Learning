{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "linear_regression_theory.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOTsiclbnuNUpPgLl1kDZE6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anirbanpranto/Stanford-Machine-Learning/blob/main/linear_regression_theory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Olr6RHE_Nvr"
      },
      "source": [
        "# **Linear Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3h2jnOq9_ML1"
      },
      "source": [
        "Linear Regression is a simple supervised learning algorithm <br>\n",
        "\n",
        "#### **Notation**\n",
        "$m : $ number of training examples <br>\n",
        "$x's : $ input variables/features <br>\n",
        "$y's : $ output variable/target <br>\n",
        "\n",
        "$(x,y)$ is one training example <br>\n",
        "$(x^{(i)}, y^{(i)})$ is $ith$ example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QN6I433ZAKBc"
      },
      "source": [
        "#### **Hypothesis**\n",
        "$$h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x$$\n",
        "\n",
        "Given a value $x$ the hypothesis gives us a $y$ value or prediction. We need to choose the $\\theta$ values cleverly so that we can get the best prediction/fit. For finding the optimal $\\theta$ values we use the idea of a cost function.\n",
        "\n",
        "#### **Cost Function**\n",
        "$$J(\\theta_{0}, \\theta_{1}) = \\frac{1}{2m}\\sum_{i=1}^{m}(h(x^{(i)}) - y^{(i)})^2$$\n",
        "\n",
        "Here, our goal is to minimize the value of $J(\\theta_{0}, \\theta_{1})$. It is called the Mean squared error function. We use this as our cost function to determine optimal hypothesis. In regression problems this is more popular. This is how we choose our $\\theta$ values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyTyt8Uh9Cce"
      },
      "source": [
        "#### **Gradient Descent**\n",
        "Gradient Descent is an algorithm to find the local minimum of a multivariable graph. It is defined as,\n",
        "\n",
        "Repeat until convergence {\n",
        "$$\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial\\theta_j}J(\\theta_0,\\theta_1) $$\n",
        "} <br>\n",
        "Here $\\alpha$ is the learning rate. In psuedocode the algorithm looks like this,\n",
        "\n",
        "```python\n",
        "def voodoomath(theta1, theta2):\n",
        "  #(theta_j - alpha*(partial derivative of cost function))\n",
        "  pass\n",
        "\n",
        "def grad_des(theta0, theta1):\n",
        "  temp0 = voodoomath(theta0)\n",
        "  temp1 = voodoomath(theta1)\n",
        "  theta0 = temp0\n",
        "  theta1 = temp1\n",
        "  if converge:\n",
        "    return;\n",
        "  else:\n",
        "    grad_des(theta0, theta1)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJSJyCfYKBjJ"
      },
      "source": [
        "### **Differential Calculus review**\n",
        "This is not part of the course, this is mostly for my own understanding xD\n",
        "\n",
        "#### **Differentiation formulas**\n",
        "* Power Rule:\n",
        "$$\\frac{dy}{dx}x^n = nx^{n-1}$$\n",
        "* Product Rule:\n",
        "$$\\frac{d}{dx}f(x)\\cdot g(x) = f(x)\\cdot g'(x) + g(x)\\cdot f'(x)$$\n",
        "* Quotient Rule:\n",
        "$$\\frac{d}{dx}\\frac{f(x)}{g(x)} = \\frac{f(x)\\cdot g'(x) - f'(x)\\cdot g(x)}{(g(x))^2}$$\n",
        "* Chain Rule\n",
        "$$\\frac{d}{dx}f(g(x)) = f'(g(x))\\cdot g'(x)$$\n",
        "\n",
        "#### **Common Derivatives**\n",
        "\n",
        "* ##### **Trigonometric**\n",
        "$\\frac{d}{dx}sin(x) = cos(x)$<br>\n",
        "$\\frac{d}{dx}cos(x) = -sin(x)$<br>\n",
        "$\\frac{d}{dx}tan(x) = sec^2(x)$<br>\n",
        "$\\frac{d}{dx}sec(x) = sec(x)tan(x)$<br>\n",
        "$\\frac{d}{dx}csc(x) = -csc(x)cot(x)$<br>\n",
        "$\\frac{d}{dx}cot(x) = -csc^2(x)$<br>\n",
        "$\\frac{d}{dx}sin^-1(x) = \\frac{1}{\\sqrt{1-x^2}}$<br>\n",
        "$\\frac{d}{dx}cos^-1(x) = -\\frac{1}{\\sqrt{1-x^2}}$<br>\n",
        "$\\frac{d}{dx}tan^-1(x) = -\\frac{1}{1+x^2}$<br>\n",
        "\n",
        "* ##### **Others**\n",
        "$\\frac{d}{dx}a^x = a^xln(a)$<br>\n",
        "$\\frac{d}{dx}e^x = e^x$<br>\n",
        "$\\frac{d}{dx}ln(x) = \\frac{1}{x}$<br>\n",
        "$\\frac{d}{dx}log_a(x) = \\frac{1}{x\\cdot ln(a)}$<br>\n",
        "$\\frac{d}{dx}e^{f(x)} = f'(x)e^{f(x)}$<br>\n",
        "\n",
        "#### **Implicit Differentiation**\n",
        "If we have a function that contains itself, we have to implicitely differentiate it.\n",
        "\n",
        "An easy example, find $y'$ when\n",
        "\n",
        "$$xy = 0$$\n",
        "$$y + xy' = 0$$\n",
        "$$xy' = y$$\n",
        "$$y'= \\frac{y}{x}$$\n",
        "We basically first differentiate in terms of $x$, then in terms of $y$ and add them.\n",
        "<br>\n",
        "Another example,\n",
        "$$x^3y^2 + xy = 3x$$\n",
        "$$(3x^2y^2 + 2yy'x^3) + (y + xy') = 3$$\n",
        "$$2yy'x^3 + xy' = 3-3x^2y^2 - y$$\n",
        "$$y'(2x^3y+x) = 3-3x^2y^2 - y$$\n",
        "$$y' = \\frac{3-3x^2y^2 - y}{2x^3y+x}$$\n",
        "\n",
        "#### **Applications**\n",
        "//todo"
      ]
    }
  ]
}